Fall 2025

>>Craig Phillips	00:00
Hey Angela. Hi.

>>awang	00:02
How's it going? Good, how was your weekend? Pretty good.

>>Craig Phillips	00:07
How about you? Did you have a good Halloween?

>>awang	00:09
Yeah, I had a Halloween party, or at a friend's, so that was fun.

>>Craig Phillips	00:15
Nice. All the other calls were on Google, on Meet, or whatever.

>>awang	00:29
Oh, your Loom notetaker says that it's not recording. Yeah, I kicked it out.

>>Craig Phillips	00:37
The Loom is not the one.

>>Drew Marconi	01:28
Hey guys. Sorry I'm late. Hey Drew. How you doing?

>>Craig Phillips	01:35
I'm good.

>>Drew Marconi	01:36
How about you guys? I'm good.

>>Craig Phillips	01:40
I think we're good. Are we all good? Cool. Yeah, thanks for taking the time, Drew. You're the final of our listening sessions. Oh, good. Grand finale. Yeah, so hopefully I've given you a bit of background on what we're doing and what we're looking to get out of this. But basically, we've just been trying to have broad conversations across the company to just get more inputs that we can then use this week as we start to flesh out a few different concepts, and both fleshing out the concepts and also doing some prototyping as well. But yeah, so I shared some of those very high -level, potentially too in -depth questions, but we can take this wherever feels right. We had a similar chat with Adam, and we went on a few journeys there. So yeah, we can take this whatever way we want to go. Yeah.

>>Drew Marconi	02:43
I mean, I have five questions from you. I'm happy to ramble. Is there one or two that you guys would want to start with? I feel like we could spend the time on any one.

>>Craig Phillips	03:03
Let me go back to the...

>>Drew Marconi	03:10
So I have, what would we do differently building from scratch today? Should we be primarily testing or experience or something else? Biggest strategic strength to double down, weakness is holding us back. Two to three biggest product issues. Yeah, I mean, I can roll.

>>Craig Phillips	03:30
We can just wait on those and we can kind of see where we go. They're all relevant to, I guess, the things we're thinking about. Yeah.

>>Drew Marconi	03:40
Okay. Rebuilding intelligence today. I mean, obviously the big, it is a very funny position to be in as a founder where I feel like we are already in the innovator's dilemma where it's like, we need to disrupt ourselves, otherwise we will be disrupted less than five years into the company. And I think that is just a result of the fact that there is pre -ChatGPT3 and all that that's unlocked and post -ChatGPT3 and all that that's unlocked from how you write code, how you position the product in different workflows, how the product is actually structured. So I think AI native would be totally part of how and what we build if we were doing it today.

>>Drew Marconi	04:47
You know, how is that different than where we are? Because this has been a big goal for us over the last year of like, hey, we have to reinvent. I'm not sure, but I do know it would be different. Like an example of positioning would be, you know, this is, I think like it runs for you and value in the background would be, would be an important framing. Like this is an autopilot. This is a, you don't need to touch anything. This is a sit back and let it, let it make you more money. I would be comfortable positioning it that way because I think there's more debating whether to like go backwards for story time. I think customers are open to that positioning now that they've heard that pitch from a lot of AI companies where it's like monitoring. It's an autopilot, it's going on. I also think we have learned over the course of Intelligems that like retention is our biggest nut to crack and like finding always on features. And so having compelling positioning there would make sense, digressing a bit. So I'm gonna like put a book in there. When we launched Intelligems and first started talking to e -commerce companies, we really thought that this would be dynamic pricing.

>>Drew Marconi	06:31
That was what Adam and I built. And we could like debate the definition for dynamic pricing all day, but like kind of a continuously evolving approach that takes in inputs, supply, demand, costs, whatever inventory and translates those into changing prices. Maybe that's over time, maybe that's different discounts for different people. That was what we had built it in ride sharing. We had this infinite number of SKUs in ride sharing, right? Like no two rides are the same. From here to the office is different at Monday at 9 a .m. Versus Saturday at 2 a .m. Versus if it's raining, if it's snowing. And so you need to like solve that algorithmically. You cannot price all those individually. You need a system that solves it. So that was like what we thought we would build with Intelligems. Some form of that adapted for e -commerce. And when we were doing these early discovery sessions with customers, with brands, we were like, hey, okay, what are your pain points? Oh, you suck at pricing. You really wish you could do better. Okay, here are like two ways to solve it. This is after we realized pricing was an issue, yada, yada. And we had two like very basic mock -ups. One was AB test the prices. Number two was like basically A Mad Libs. Hey, I want to maximize this metric for this set of products within constraint one that conversion rate shouldn't fall below this and constraint two that the inventory should sell through by whatever. And we can just solve that problem for you. And we thought everyone would go for number two. Everyone was like, just let me test it. Like just let me be in control, pick these ones.

>>Drew Marconi	08:14
That's the information I need. And that was like, I'm very grateful we did those discovery calls then with the mock -ups because that was completely counter to our expectation. And it's what we built obviously. And there have been like three or four dynamic pricing for e -commerce startups that have like come and gone in the interim. They like try to start with this really complicated build the whole model, make it dynamic. And then they get no pickup. And like, I think the fact that we took that feedback and gave our customers control and let them say, hey, A versus B versus C is a big part. It was an important choice. And so, but that is like a belief. Now, why did the customers want to do that? I think a big part was trust and like control. The reason I'm saying this is I feel like that assumption is a little bit like customers today would be a little bit less, care about that a little bit less because A, they know Intelligems And they know us and like they trust us And they've seen the data. B, they have been now like marinating in this context of Ai tools and they can do it for you. It's so smart and they're using it and they're seeing impacts probably in some small parts. So there's just like more market acceptance. I still think testing would be a part if I were rebuilding it today because like some people that's how they're gonna build trust and like, it's a great thing.

>>Drew Marconi	09:46
Like it's a great entry point for me. But from early on part of the positioning being, hey, we're testing into a longer term strategy for you. We're testing you into first it's test, then it's co -pilot, then it's autopilot. I mean, we've never been surprised that retention has been a challenge. Like we knew that was clear that like testing products have challenges with retention from day one. Like Optimizely had exist. It was like the main piece of objection I got when raising our pre -seed round and then our seed round. And like, you know, So it's not shocking But I think we would have gone harder. I would go harder and faster at like solving that problem in parallel with building out testing. What else would I do differently from scratch? I think like, I don't know how to solve this but, and this kind of relates to the second question because we had price testing and then we spent so much time building content testing and they became different plans with different expectations of the market. We present them. I think even us all see them as juxtaposed. It's like this versus this, this or this. And that was like, even when I was raising our series A it was people were like, are these two businesses? And I think that like that dichotomy actually shouldn't be that strong.

>>Drew Marconi	11:32
Like I don't see price testing and content testing as different products fundamentally or like different business lines. Like they're priced differently. They captured different entry points for customers but I would wanna find a way to position. I think we're trying to do this now where it's like, hey, this is all the same thing. Like brand coming in, you may be telescoped into this part of the world, but open the aperture. This is all part of it. Hey, you may think you really wanna test this price. Actually you need to like run a more optimized business and like use experimentation more. So I don't know where, what exactly that'd be different but like, I think we juxtapose these too much and need to preach about like, this is all the same thing. It's Cro needs to broaden its definitions and like, but that doesn't mean Cro is not important. Like that's another thing in our positioning. Like we kind of end up in this funny world where sometimes I feel like we like have to put down Cro but like, we're very good at that. And like, visual tests sometimes are really powerful. Okay, I'm gonna stop on that. Go to question two. Should intelligence primarily be a testing platform and experience personalization platform or something else?

>>Drew Marconi	13:14
I think the less juxtaposition we can create here, the better. I believe that testing is always going to be a core part of intelligence. Why do I believe that? Testing is an existing category. There's a huge amount of demand and knowledge and know -how that's been built over 30 years of A -B testing online. People want a tool for it. It's a big category that we can like use as our kind of foundational layer. It serves, so it serves as a wedge into customers. They know they need a testing tool. When you want to test, it's so urgent. Like people come to us so desperate to get an answer which is an incredibly powerful like sales and onboarding and growth tool of like, hey, I am selling you an answer. We can create data out of nothing to help you make better decisions. Like people literally could not get data on pricing decisions before we existed. And it's like, we can create that. That's awesome. And so that's like a really compelling thing to be able to sell. I'm gonna help you understand the answer. It then serves, testing serves as our wedge into like different parts of the site. Like, hey, let's go test your checkout page. Oh, it turned out that this like module worked. Why don't you roll it out with us? Thanks.

>>Drew Marconi	14:46
Out with us. Hey, like, you are testing your offers. It turns out, like, bundle works well. Why don't you keep using that with us? And so, that strategy is, like, still very much early days and in motion. But, like, it's our wedging the customer and that can be our wedge into these different teams. Because, like, a well -run company, all of these teams are gonna have testing needs in all these parts of the site. And, yeah, I mean, it's just, like, kind of dramatic. Like, it's A versus B and before and after. So, it also, like, forces us to be really, really disciplined and rigorous on analytics, which I think is key. Now, this is why there's not a juxtaposition. I think testing alone, you end up, you can build a good business, but greatness is gonna be, hey, challenging because of on, off this, that. So, we also need to be able to, like, the way I visualize this is there's, like, testing is this sawtooth pattern of value, right? You run a test. Oh, I found an answer. Great. I captured all this value. I roll it out. Now what? I'm getting no value from intelligence or this testing platform in the meantime. Okay, great. I need to run another test. I need to think of it, put in this effort, design it, do it. Great. I captured a bunch of value from running this test. And we need to use experience and personalization to fill in the gaps there. We need to build a steady baseline of value creation for our customers.

>>Drew Marconi	16:27
Fully possible. We know that that can be done through personalization. We see the experiences that people are running. When we run tests, we get insights about which audiences prefer which things. So, I think, like, it's a hard move, but, hey, you're testing. Now use us to build in this foundational value layer that's just, like, a rectangle that's always filled in. You are always making money from intelligence, and, hey, it spikes when you run these tests. And I think that that comes with experience personalization. I think that that can come through, like, analytics adds a piece to that. I think that there are probably some, like, dynamic offers and dynamic pricing. The reason I'm so bullish on that is that's a huge chunk of steady, consistent value. And, like, Ai, when I think about agentic CRO, like the flywheel, the agentic flywheel, what that's really doing is compressing these sawtooths. So, like, right now, when it's all human -led, you may have a decent gap in between each of these winds. Agentic Cro is just, like, we increase the frequency and, like, trying to push those teeth together. So, it's still really important. It's still very valuable. But I'd be, like, how can we compress and make more frequent the, like, increase the amplitude and the frequency of this testing value curve while also building very, this would be but building this base layer. How that choice should shape the product, you know, I think it's about breaking down barriers and, like, less juxtaposition.

>>Drew Marconi	18:12
Like, that's very vague. But it's just so discreet in the app right now. And we don't want to confuse people. If you're coming in, you want to do this thing, it should be easy to find and do this. But, like, we need to show people how this is all related. How when running a test, you're going to get an experience to roll out. You're going to get suggestions for personalization. So, you go and roll that here. And you then see, hey, all of these things are adding up to extra value.

>>Craig Phillips	18:42
Can I ask about dynamic pricing? I know it's probably a whole topic on its own. But I'd be curious, like, your thought on dynamic pricing and how, like, because that was an early idea that, I don't know how much has been explored, what that means for us. But I'd be curious, like, what's your, like, elevator pitch on dynamic pricing or, like, the quick sort of summary of how you approach it?

>>Drew Marconi	19:06
Yeah, I think dynamic offers that Michael is working on is dynamic pricing. Let me pitch it. I have two pieces of content I'm going to send you guys after this. Dynamic pricing is kind of, like, a bad word. Like, and people all have, we all have our own assumptions that we bring into it. I'm going to use a much more broad definition, which is, like, dynamic pricing is having an automated system that looks at inputs, looks at a goal you're trying to achieve,

>>Drew Marconi	19:46
Goal you're trying to achieve with pricing and suggests changes automatically. It does not need to be different prices for different products, different prices for different people. It doesn't need to be fully automated. It's just like, I'm going to take into account inventory and use that to change the discount levels of my product. So like brands who look at clearance with their merchandise and their buyers, that is dynamic pricing. Hey, this is selling really slow. I'm going to add it to the clearance section. It's just really f****** slow and being made by decisions that are like maybe inconsistent, maybe being made from a spreadsheet at best or off the merchandisers vibes. And so how can we make those decisions faster for brands? Now why is this valuable? Well, like anytime you are mispricing a product, you're losing efficiency and potential profit. So like if the product is priced too high, okay, you're going to get some customers, but like you're going to be stuck with leftover inventory that you paid to purchase. You put cash out, you're now not collecting back. I'm going to either need to discount it or drop it to like get there. If you're priced too low, like, hey, there's actually, you can make more profit by having a higher price point and capturing more margin from like other folks. This is one of the pieces, I wrote this better in one of our like founder newsletters, but basically you're getting closer to the right price.

>>Drew Marconi	21:30
I'm skeptical that the right price is constantly changing. And the better you are at doing that, like you're a better match in the supply demand curve and you're going to capture more profit. The reason why it's really compelling as a business model for us is number one, if you get better at pricing, it immediately flows down to the bottom line. Like no ifs, ands, or buts, very clear, I priced better, we made more profit. Number two, it's never a skillset that e -commerce brands are going to bring in house, especially like, I mean, maybe if you're Best Buy and Home Depot, but like, you're just not going to get data science to do this. And what that combines to is we can generate a lot of value because we're doing a better job and it's very easy to illustrate, like we could have a holdback test and say, hey, our algo is like producing way more. And it's going to be very hard to compete with, which means we can like really take a chunk of the value that we're creating. Scattered point kind of Separate, like it's very hard to take a high chunk of the value we create in just A -B testing. Like if I think about what our pricing power is, because, OK, great, we ran this test, it found $200 ,000 of extra profit per year. But like who, we just measured it, right? We didn't come up with a design. We didn't come up with the idea. We didn't, you could go test it another way or could roll it out. So like we can't, that $200 ,000, we want to show that, we want to show that this practice is valuable, but like intelligence didn't create that. Whereas if you pressed go on dynamic offers and we are like now getting you eight extra cents of profit per visitor, you turn that off, you don't have an alternative.

>>Drew Marconi	23:12
You're back to the old days where you were making eight cents fewer. So we can charge two cents from each visitor as long as we can clearly illustrate that value. So from our economic model perspective, it's like significantly more powerful. The last thing I'll say on dynamic pricing is like pricing is far more than just the list price. I know you guys know this, but that can be the shipping rate, the offer, the volume discount, the shipping threshold, the subscription rate, like all of that stuff that goes to is the customer going to buy and what is hitting my credit card is part of pricing. And like, I think actually because there's so much fear and uncertainty and doubt about like dynamic list prices, that's why we've chosen to go after shipping thresholds and offers first. Like if you guys, if I was like, hey, would you be okay with a restaurant charging you like different amounts? I'd be like, no, that is really annoying. But if me and Milena are sitting on the couch and get different Uber Eats discount codes, we're like, great, let's use a cheaper one. Like personalized discounts is just much more palatable in the U .S. in 2025. I can hear you. I'm going to have to take bread out of the oven, but I'm with you. Cool. Was that like a, did that answer your question, Craig?

>>Craig Phillips	24:31
Yeah, totally. I think it's, yeah, it's, it's a. Yeah, it's interesting to think about the impacts that could have to to incorporate that more. Cool.

>>Drew Marconi	24:46
So.

>>Drew Marconi	24:46
Um, in terms of strengths, um, and weaknesses, uh, I think our data pipeline and rigor and accuracy is a huge strength. Like we've processed an incredible amount of data, which in and of itself isn't a strength. I mean, there's like literally billions and billions and billions of dollars of transaction data in our data set that we like don't use. Um, we've like pretty much never dropped a large amount of data or had inaccurate data. We may be querying it incorrectly or like showing a slightly different view than what we'd expect, but like it is there, it is accurate. And, um, our team and our product are really good at, at, I think like building that. Uh, and, and that's part of the reason why I'm just like excited for sitewide analytics as a starting point to just, let's make more use of the strength. Maybe it becomes a thing, maybe it's an evolution, but like the way I frame it is we have whatever $50 billion of transactions in our data set. We only ever show you these very narrow cross sections, these slices. When you have a tech, you may only have a test running on 20 % of your traffic, three groups, and we're showing you these, we have so much else to show you.

>>Drew Marconi	26:34
And we've never brought that to bear to customers. We just take these like, like boring samples, very tiny slices, instead of showing you the bigger picture in addition to what you're testing. Um, so that's a strength.

>>Kate White	26:50
Um, sorry, can I ask a quick question on that one?

>>Drew Marconi	26:52
Yeah.

>>Kate White	26:53
I know you're not talking moats, you're talking strengths, but I have a general curiosity around why that is such a strength of ours versus any other, like, why don't our competitors have the, the depth of data that, that we have?

>>Drew Marconi	27:10
I mean, they, they may underneath. Um, I think, uh, part of it is Adam had built extremely high volume data pipelines in his last job as an Andrew. So like from a technical infrastructure perspective, it was like very robust from day one. Um, I think our approach is always like, let's just gather more data. It may be useful later. Like, this is part of the reason why I'm like very excited. We're finally getting to the semantic understanding of test work, because that's basically data we've been losing for three or four years. Cause it's hard to recreate going back, but like, let's collect more data. Now tech was only going to get better. More questions will come. We want to be able to like potentially bring it to bear to an answer. Um, Shopify plays a huge beneficial role here of like, we can subscribe to webhooks from Shopify. And so we get like enterprise grade data feeds in, um, that are in a very similar structure, uh, to each other. So that's actually like a barrier to going outside Shopify solvable. But, um, yeah, I think it's kind of just like experience and culture. Our last company was just highly, highly instrumented. Like, you know, what was the exact profit and loss on an individual ride where five people got in, four people got out, we hit traffic, we had a toll, we went to the tunnel. And then like, what promo was this driver on? Were the riders using promos? And like, there was no way to, you needed to have a huge layer of ground truth in order to like understand anything.

>>Drew Marconi	28:54
Um, so I think that was just our bias. Got it.

>>Kate White	28:57
Got it. So it was implement, it was an intentional implementation tactic from the start is kind of, okay. Yeah. I mean, cool.

>>Drew Marconi	29:04
And part of like, to answer pricing questions, we just needed our hands on a lot of stuff. That's part of my content testing was relatively easy. Like analyzing a price test, you need to know like the profitability, the cost, the Aov, like this whole picture. And then understanding if the landing page performed better or not, it was just like a subset of that, of the superset we'd already gathered.

>>Emily Brown	29:28
In general, in the Shopify ecosystem, there can be a lot of competition, but there's always a certain subset of that competition that is just not technically very deep. And that's just one of the dynamics of operating here, which, you know, if I

>>Drew Marconi	29:46
You know, if I, I don't know if this is a different strength or if it's positioning comment and obviously take all of this with massive palmfuls of salt. There is a framing of like, why would someone use our volume discount? Or why would someone use the Intelligems progress bar? Or, you know, even imagine we just roll out like an announcement bar feature and it's like plug and play, like, why would I use this? There's like a free version, there's whatever. Now I'm like locked in. I think our, the why there is this is the smart version. Yes, at the surface level, it may look the same. What you know you will get with anything that Intelligems rolls out, it is testable. It is personalizable and you will have amazing data about it so that as your tools get smarter, as you get smarter, it will like scale with you. So if you want to just have one thing for everyone, maybe you don't need us. But if you want to understand it, get more value, iterate on it, test it, then like ours is smart by default. And I think that's our edge as we push from more and more like pixels on the site and more and more control. Hey, this is the smart and analytical version of X, Y, or Z. I think a strategic weakness, I mean, I'm sure I could think of way more of these. There's a difficult balance. Like, I think we have a lot of nuanced thinking. Maybe another strength of ours that comes with this weakness is like configurability, robustness and like the ability to say yes.

>>Drew Marconi	31:36
Like our attitude pretty consciously from day one and anyone I'm happy to go a bit longer has been great, you want to test that? Let's try to make it happen. Like, that's how we went from pricing to shipping to offers to this. Like, that's been very much part of the ethos. And I think it served us very well. And like, I don't want someone to go plug in a different tool to test some of that. No, like do it with us. Similar to how we like built with all the data available. I think we've built in such a way that it's like pretty configurable in a reasonably robust way. Like, even if we're hacking s***, it's still reasonably stable. Like, it's not like, oh, this is going to break because it's like completely over on the side. It's like plugging together parts that already exist. So modular maybe is the right word. And we've all been part of like annoying deals that have stretched the limits of that. But I do think like, and you can do it via Api and you can do this thing with Css or JavaScript. Like that configurability, I think lets us build faster, lets our customers build faster. It is really like resonates with our larger brands who have enterprise teams. I think we could like double down on that further by thinking about how do we open up the platform way more? Like I'm really excited by the Api work. Cause like, yeah, let's let people like build s*** on this and hack it in their own ways. Even like the prompts engineering and flow framework that like Christian put together. It's like, what if we just like gave that to customers? Like what would they do with that? Like just putting more tools in the hands of our power users, I think can create some magic.

>>Drew Marconi	33:21
And like personalizations, for example, came as a result of seeing a customer hack us. We were like, wait, why is this test like zero percent, a hundred percent with this weird targeting? And he's like, oh, Well I only want people to get this discount if they come from this link. And it was like, oh, okay, cool. So configurability, robustness, like making a tool that works great for power users, we could totally lean into. I do think then like balancing that with simplicity is hard. And it's one of our weaknesses is that we have many ways of doing something and want to present a complete view to customer like, well, you can do this with Css or you can do it with a template test or you can do it with a theme test or you do it. And like sometimes the way we talk to customers and show them is more about the different ways of how to do it and a little bit less about like, here's what we want to do, great, we'll just do it for you. Like we don't, we give many ways that you could do it instead of just saying, just do it this way. Hey, if you want more detail, here's like an advanced mode. And I think giving AI more keys to control the platform like solves, maybe just like make some of this moot because it can determine the best thing and run on the tracks. But like, yeah, it's a balance that's hard in like,

>>Drew Marconi	34:46
That's hard in like calls. Like I had lunch with an agency guy on Friday. He was like, how would you set up this like buy box tests? And I caught myself. I was like about to give him three different options. I was like, let me just pick one. And then he had questions and challenges to that. So I was like, great, we have, we can explore this other one. But I think that also comes through in the product at times. Like even, hey, I want to launch a content test. Well, which of these four types? And it like, I don't know, like when we look at product metrics, like I don't think of theme, template, Css, or redirect. Those aren't relevant like metrics about what's being tested. Because I could run a landing page test in any four of those. I could run a cart test in three of those. So like just the way we present that, it's more about our features versus what the customer is trying to accomplish. I will finish my ramble. Product issues driving churn and poor activation. I mean, I think this is part of it. I think like it's intimidating, especially if you haven't been sold and like you haven't been onboarded. I think like site speed has been top of mind a lot recently. It's never going to be fixed. Site speed is kind of the start guard, right? Like what's slowing down a site? What metrics matter? How is it on different devices, different cell networks? There's no true truth on site speed and site speed impact. Customers reach out to us. They're like, my site seems slow. I think this is hurting my conversion rate.

>>Drew Marconi	36:28
And we actually have pretty good tools internally to diagnose and calm people down or make some changes and improve it. And we benchmark a theme with us versus a theme without us. There's a pretty good process in place to understand and handle it. You only get access to that if you reach out probably complaining. And it's like if we know that this is a big concern for customers, we know that it's a very fuzzy, fudgy problem, we are like waiting to be put on the back foot by customers. And like our competitors have started like sell on this point because it's just like kind of vague fear, uncertainty, versus like we could own it even if it was an animation that just said like, hey, we're checking on your site speed. We're doing this. But we don't have a perspective on what it should be, how you should look at it. Is it OK? Can you check it? And it doesn't need to be available for everyone. But it's one of those problems that we've looked at strictly as a technical and operation problem. And we haven't looked at it as a product opportunity. And I think, I don't know, we surely could look up how often it comes and like, is it probably the biggest churn driver? Probably not. But like, I think it's emblematic of this thing where we handle it as an objection instead of being like, let's be front footed, like, let's own this. Run your check with us if you care about it. Like, oh, you have questions, go hit this button and like, talk to us. Because the other thing is like, I'm still surprised at how many times we see people churn and we look at s*** And they're so confused and they never just like reached out to talk about it. And we have the most helpful expert team available.

>>Drew Marconi	38:12
And we invest a ton of our revenue into great support and success teams. But like, people just don't reach out. So it's like, when are we waiting for things to go wrong and then hoping that someone reaches out so we can impress them with our response versus like, where can we say how things should be going? And if something looks off, please reach out and we're happy to solve it with you. And like, invite it rather than, with a perspective, rather than just like waiting for people to complain. I don't know, I mean, I feel embarrassed that I don't have more product issues. That's one that I've just been like noodling on a bit because it has this like kind of producty culturally part.

>>Emily Brown	39:03
Drew, is this a, I put this question in the chat, but is this like a concern that only popped up once we started doing content testing or were people worried about speed? No, it's always been. It's always been there. So it's just kind of like an existential.

>>Drew Marconi	39:18
Yeah, I think people know that, people know that testing of any kind can impact site speed. Everyone's had some bad experience or knows someone who knows someone who did, just in the ether. And there's been a lot of research. There's like some big report in 2021 or 2022 of just like site speed equals more conversions.

>>Drew Marconi	39:46
Now that I'm talking about it, that was also when a bunch of these like site speed improvement products, be it headless sites and this came out, so like who knows how good the research was versus just funded by people advocating trying to sell you s*** for faster sites. But like there was just a lot of like, you need to be worried about your site speed injected. And so it is like broadly just an anxiety -inducing topic because it's very hard to control. Lastly, how should AI shape the product roadmap? Yeah, to me, like making the flywheel of testing agentic is huge. I want us to be moving faster. I know we have the breakdown of the different pieces, but like I think the biggest blocker, I mean, but let's call the biggest product issue just efforts. Like that's what caused the churn. It's just it's effortful. You have to think of a test. I have to go design. I have to figure out how to f****** use all the configurations, set the targeting. I have to QA it. I have to like QA it on mobile. I have to get okay from my team. I then have to run it. I have to check it after an hour to make sure it's doing well. I have to check it after a couple of days to make sure the data is doing well. I have to go analyze it, put together the Jack, advocate to my team again. We decided we're going to roll out this. I have to go have my dev code it and like part in it. That is a massive amount of efforts, which makes it easy to turn when you're in one of these troughs in the sawtooth pattern. It like, you're just like, ah, it's I have other stuff that's more important to do. So like, it's just effortful.

>>Drew Marconi	41:28
And I think that having Ai like reduce effort through that whole flywheel is really huge. What I do think will be important, but I have less of a strong perspective on right now is like how people shop where traffic comes from is going to change is in the early days of changing. I think like it's very, very early. And so I'm not ready to like place a big bet around that, but it's certainly something that we need to like keep an eye on because in three years, I imagine it will hit our roadmap. And then the dynamic offers, dynamic pricing, I think is like old AI, you know, it's like ML. It's not, I don't think we need generative LLM approaches there. It's just like kind of classic data science. That was a big monologue. I'm like happy to share follow -up questions, dig into anything.

>>Craig Phillips	42:33
That's great. No, thanks for digging into all those questions. Yeah. Does anybody have any specific follow -ups on any of that or nothing specific? Cool. Well, thanks for everyone. Thanks for going over time. We've got all the recordings and notes we'll throw in and bring into the context of all our thinking on this, but Yeah, We might reach out with more details, but this was really, really useful. So great.

>>Drew Marconi	43:06
Cool. See you guys in a couple of weeks. Cool.

>>Craig Phillips	43:09
Thanks.